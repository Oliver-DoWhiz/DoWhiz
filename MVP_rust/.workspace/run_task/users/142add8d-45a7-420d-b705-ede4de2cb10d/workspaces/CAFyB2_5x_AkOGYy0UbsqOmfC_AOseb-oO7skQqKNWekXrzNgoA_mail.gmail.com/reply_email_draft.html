<!DOCTYPE html>
<html>
  <body>
    <p>Hi Mini,</p>
    <p>You are right - I found the earlier email with the SkillsBench paper. I have attached a markdown conversion of the paper and included a concise summary below.</p>
    <p><strong>Summary (SkillsBench: Evaluating Procedural Knowledge for AI Agents)</strong></p>
    <ul>
      <li>Introduces SkillsBench, a benchmark for evaluating procedural Skills (instructions plus resources) as first-class artifacts, with 85 tasks across 13 domains, deterministic verifiers, and trajectory logging.</li>
      <li>Across 7 agent-model configurations, Skills generally improve pass rates but with wide variance (roughly +7.5 to +19.9 percentage points); 24 of 85 tasks show negative deltas, and software engineering drops about 5 points.</li>
      <li>Harness behavior matters: some systems reliably use Skills (notably Claude Code), while others often acknowledge but underuse them (e.g., Codex CLI), affecting gains.</li>
      <li>Design matters: 2-3 concise Skills per task work best; too many or overly comprehensive Skills reduce benefit.</li>
      <li>Full Skills with scripts/examples outperform minimal or self-generated Skills; self-generated Skills provide only small improvements.</li>
    </ul>
    <p>If you want the markdown cleaned further (tables/figures, layout), or a deeper summary focused on specific sections, tell me and I will refine it.</p>
    <p>Best,<br>Oliver</p>
  </body>
</html>
