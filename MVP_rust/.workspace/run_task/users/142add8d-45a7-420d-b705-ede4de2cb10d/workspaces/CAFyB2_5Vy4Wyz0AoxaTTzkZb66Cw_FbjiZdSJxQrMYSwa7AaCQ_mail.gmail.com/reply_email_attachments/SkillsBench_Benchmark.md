# SkillsBench: Evaluating Procedural Knowledge for AI Agents
**Authors:** AnonymousAuthors
## Abstract
We introduce SKILLSBENCH, the first benchmarktosystematically measure effectiveness of Agent Skills as first-class evaluation artifacts. AgentSkillsarestructuredpackagesofprocedural knowledgeandresourcesthatpromiseLLMagent capabilities gains without model modification. AlthoughAgentSkillsareproliferatinginreal-world agentic systems, there is no standard method to evaluate their effectiveness. SKILLSBENCH directly measures Skill efficacy by comparing task performance with and without Skills across agentmodel combinations. We curate 85 tasks spanning 13 domains with deterministic verifiers and Figure 1. Agent architecture stack. Foundation models provide trajectory logging. Across 7 agent-model conbase capabilities, agent harnesses orchestrate tools and context, figurations and 6970 trajectories, we find Skills andSkills extend competence to specialized domains. This layered improve performance by 19.9 percentage points design enables modular augmentation without model modification. onaverage. However, 24 tasks show negative effects; Software Engineering performance drops damental tension exists: foundation models provide broad by5points with Skills. Our analysis reveals that capabilities but lack the procedural knowledge required for a small number of compact Skills is optimal, and domain-specific workflows, while fine-tuning is expensive that Skill-aware harnesses suffer significant reand sacrifices generality. liability issues. These findings establish design principles for Skill authors and highlight the need Agent Skills offer an emerging solution. A Skills is a strucfor domain-specific deployment strategies. tured package comprising instructions, code templates, resources, and verification logic that augments agent behavior at inference time without model modification (Anthropic,
1. Introduction 2025a). Skills encode procedural knowledge: standard operating procedures, domain conventions, and task-specific Large language models (LLMs) have evolved from text genheuristics that guide agent behavior. This modular aperators into autonomous agents capable of executing comproach builds on the options framework for temporal abplex, multi-step tasks in real-world environments (Brown straction (Sutton et al., 1999) and cognitive architectures for et al., 2020; Chowdhery et al., 2023; Touvron et al., 2023; language agents (Sumers et al., 2023), mirroring successful Ouyang et al., 2022; Yao et al., 2022). This evolution is computing paradigms: foundation models provide base caexemplified by agent-centric CLI tools: Claude Code (Anpabilities (analogous to CPUs), agent harnesses orchestrate thropic, 2025b)fromAnthropic,GeminiCLI(Google,2025) context and tools (operating systems), and Skills extend fromGoogle,andCodexCLI(OpenAI,2025)fromOpenAI competence to specialized domains (applications). enable developers to leverage frontier models as agentic Skills ecosystems have grown rapidly, with community assistants within terminal environments. However, a funrepositories now hosting thousands of user-contributed 1AnonymousInstitution,AnonymousCity,AnonymousRegion, Skills spanning software engineering, data analysis, and enAnonymous Country. Correspondence to: Anonymous Author terprise workflows. Yet despite this proliferation, no bench- <anon.email@domain.com>. marksystematically evaluates how and when Skills improve Preliminary work. Under review by the International Conference agent performance, what content drives gains, or what deonMachineLearning(ICML). Donotdistribute. sign principles distinguish effective Skills from ineffective
**Table 1. Comparison of runtime augmentation paradigms.**
ones. The question is not whether adding task-relevant context helps, but rather: How much do Skills help compared Prompts RAG Tools Skills to baseline augmentation? Which Skills components (instructions vs. code vs. examples) contribute most? When do Modular/reusable     Skills fail despite being present? Procedural guidance Limited    Executable resources     Existing agent benchmarks (Liu et al., 2023; Merrill et al., Cross-model portable     2026; Jimenez et al., 2024; Zhou et al., 2024b; Xie et al., 2024; Koh et al., 2024; Trivedi et al., 2024; Yang et al., not a single instance 2023; Chan et al., 2025; Zhuo et al., 2025) evaluate raw  Structuredcomponents: IncludesaSKILL.mdfileplus model capabilities in isolation, answering "How well can optional resources (scripts, templates, examples) this model perform task X?" but not "How much does Skills  Portability: Skills are soley based on file systems, so it's YimproveperformanceontaskX?"Thisgaphaspractical easy to edit, version, share, and be used across different consequences: practitioners cannot make informed deciskills-compatible agent harnesses. sions about Skills adoption, and researchers lack empirical grounding for Skills design principles. This definition explicitly excludes: system prompts (lack Toaddress this, we introduce SkillsBench, the first benchstructure and resources), few-shot examples (Brown et al., markthat treats Skills as first-class evaluation artifacts, with 2020) (declarative, not procedural), RAG retrievals (Lewis three core contributions: et al., 2020) (factual, not procedural), and tool documentation (Schick et al., 2023; Qin et al., 2024) (describes ca-
1. A Skills-centric evaluation framework. We curate 85 pabilities, not procedures). We acknowledge this boundary
tasks across 12 domains, each executed under vanilla and is not absolute (for example, a StackOverflow answer may Skills-augmented conditions with deterministic verifiers blend factual and procedural content), but our criteria proand full trajectory logging. We stratify tasks by diffivide operational clarity for benchmark construction. We culty and conduct leakage audits to ensure Skills provide highlight the distinguishing features of Skills compared to guidance rather than solutions. other augmentation paradigms in Table 1.
2. Large-scale empirical evaluation. We evaluate 7 agentIn SKILLSBENCH,eachSkills is a modular package located
modelconfigurations across 6970 trajectories, producing in environment/skills/ containing:  SKILL.md: the first systematic evidence on Skills efficacy, variance, Natural language instructions specifying how to approach and failure modes. a class of tasks, i.e., workflows, standard operating proce-
2. SKILLSBENCH dures, or domain conventions.  Resources: Executable
scripts, code templates, reference documentation, or worked We present SKILLSBENCH, a benchmark for evaluating examples that the agent may invoke or consult. the efficacy of Skills augmentation in LLM-based agents. Built on the Harbor framework (Merrill et al., 2026; Harbor 2.2. Task Specification FrameworkTeam,2026),eachtaskadoptsacontainerized structure with an environment that includes agent Skills Each task in SKILLSBENCH is a self-contained module and related data, a deterministic verification test, and an comprising four components: oracle solution. Following best practices for agentic bench-  Instruction. A human-readable task description specifymarks (Zhu et al., 2025; Anthropic, 2026), we ensure strict ing the objective, input format, and expected output. We isolation and deterministic verification. Unlike Terminalwrite instructions to be solvable by a knowledgeable huBench, which evaluates raw model and agent harness camanwithout access to the paired Skills, though the Skills pability, SKILLSBENCH introduces a key methodological maysubstantially reduce time-to-solution. difference: we evaluate every task under both vanilla (no  Environment. ADockercontainerwithtask-specificdata Skills) and Skills-augmented conditions, enabling direct files and a skills/ subdirectory containing modular measurement of Skills efficacy. Skills packages. The container ensures reproducibility through isolated dependencies and clean file system state.
2.1. Skills Specification  Solution. A reference implementation demonstrating the
task's resolvability. This oracle validates that each task ASkills is an artifact that satisfies four criteria: has at least one correct solution path.
- Procedural content: Contains how-to guidance (proce-  Verifier. Deterministic test scripts with programmatic asdures, workflows, SOPs), not factual retrieval sertions, including numeric tolerances where appropriate.
- Task-class applicability: Applies to a class of problems, This ensures reproducible pass/fail determination without
LLM-as-judge variance, following execution-based evalu-
**Table 2. Task difficulty stratification based on human completion**
ation best practices (Wang et al., 2023b; Brown, 2025). time.
2.3. Dataset Construction Difficulty Tasks HumanTime
The expressive and flexible nature of Skills and our task Core 17(20.0%) <60min specifications enables broad coverage across diverse doExtended 42(49.4%) 1-4 hours mains and problem types. To maximize this diversity, we Extreme 26(30.6%) >4hours adopted a community-driven, open-source contribution model: XXcontributors from both academia and industry submitted XXX candidate tasks. We count submissions that included the full task specification (instruction, environalso evaluate instructions by six criteria (explicit outment, solution, and verifier), along with author-assessed put paths, structured requirements, success criteria, condifficulty ratings. From this pool, we curated the final straints listed, context-first ordering). SKILLSBENCH dataset through a rigorous two-stage selection process. (TODO: more elaborations here). HumanReview. Afterautomatedcheckspass,maintainers conduct manual review evaluating five criteria: (1) data
2.4. Quality Control validity: input data must reflect real-world complexity; syn-
2.4.1. CONTRIBUTING PRINCIPLES thetic or toy data is rejected unless justified; (2) task realism: scenarios must reflect realistic professional workflows Contributors must satisfy explicit requirements designed to without artificial difficulty; (3) oracle quality: reference ensure task quality and prevent shortcuts: solutions should match how domain experts would solve the task; (4) Skill quality: Skills must be error-free, internally Human-AuthoredInstructions. Task instructions must consistent, and genuinely useful for similar tasks beyond be written by humans, not generated by language models. this benchmark; (5) anti-cheating: tasks must prevent shortWeenforce this because LLMs generated queries will be cut solutions (editing input data, extracting answers from confined by the distributions of LLMs, which are the subject test files, exploiting verifier implementation). Reviewers of our evaluations, and LLM-generated queries are most of run benchmark experiments with and without Skills across the time of low quality. multiple agents to confirm each task provides meaningful signal about Skill efficacy. Skill Generality. Skills must provide procedural guidance for a class of tasks, not solutions to specific instances. Instructions must not reference which Skills to use, meaning Leakage Prevention. To prevent Skills from encoding agents must discover and apply Skills autonomously. This task-specific solutions, we enforce explicit authoring guideensures we measure genuine Skill utilization rather than lines and conduct leakage audits. A Claude Code Agent instruction-following. SDK-based validation agent runs in CI to detect potential Skill-solution leakage; tasks that fail are rejected. Skills Deterministic Verification. All success criteria must be must NOTcontain: task-specific filenames, paths, or identitestable through programmatic assertions. We target minifiers; exact command sequences that solve benchmark tasks; malnumberoftestsneededforverification, avoiding both constants, magic numbers, or values from task specificainsufficient coverage and redundant test bloat that leads to tions; references to specific test cases or expected outputs. artifical low pass rates. Tests must include informative error Skills must apply to a class of tasks, not a single instance; messages and use parametrization rather than duplication. provide procedural guidance (how to approach), not declarative answers (what to output); and be authored indepenAutomatedValidation. Each submission undergoes autodently of benchmark specifications. mated validation before human review:
- Structural validation: Required files present 2.5. Benchmark Composition
(instruction.md, task.toml, solve.sh, SKILLSBENCHcomprises85tasksacross 12 domains, with test_outputs.py), correct directory layout, valid category distribution shown in Figure 2. We stratify tasks TOML/YAMLsyntax. by difficulty, measured by estimated completion time byOracleexecution: Referencesolutionmustachieve100% individuals whom we consider median specialists for the
test pass rate. Tasks with failing oracles are rejected. tasks, without the assistance of AI tools. Original taskInstruction quality: instruction must be human writcontributors provided human time estimates, reviewed by
ten (we apply both human review and GPTZero review, an additional set of reviewers from the maintainers who are and we achieve human label on 100% of our tasks). We expert in the same domain.
### 3.3. Models
General Weselect six frontier closed-source models: GPT-5.2 (OpeData Processing nAI), Claude Opus 4.5, Claude Sonnet 4.5, Claude Haiku 17 Scientific Computing 5 Security 4.5(Anthropic),Gemini3Pro,andGemini3Flash(Google), 5 Multimedia plus one frontier open-source model: MiniMax-M2.1. All Financial models use temperature 0 for deterministic sampling. 6 11 Control Systems Document Processing We evaluate each model using its compatible agent har- 7 Software Engineering 7 7 10 Planning & Optimization nesses. Claude Code runs with all four Claude and MiniManufacturing Maxmodels;GeminiCLIrunswithGeminimodels;Codex Web Performance Healthcare CLIruns with GPT-5.2. This yields 7 commercial modelharness configurations for main experiments. Terminus-2
**Figure 2. SKILLSBENCH consists of tasks spanning 12 domains. is used separately for ablation experiments (4). The full**
configuration matrix is in Table 9.
### 3.4. Skills Resolutions
For main experiments, we evaluate each task under two
3. Experimental Setup conditions: L0 (no Skills) and L3 (full Skills). For abWeevaluate three commercial agent harnesses on SKILLSlation experiments (4), we additionally test intermediate BENCHacrossseven frontier models. For each supported resolution levels: modelandharnesscombination, we run the benchmark at least five times per Skills condition, resulting in 2857 valid  L0(NoSkills): Agentreceivesonlyinstruction.md, trajectories. A trajectory is valid when the agent passes, noSkills present in environment. fails, or times out on a task without infrastructure and run-  BYOS (Bring Your Own Skills): No Skills provided, time errors. Each trajectory is one agent's attempt at solving but the agent is prompted to generate relevant procedua single task under a specific Skills condition. ral knowledge before solving the task. This isolates the impact of LLMs' latent domain knowledge.
3.1. Terminus-2  L1 (Minimal): Function signatures and installation instructions only (6% of full content). Because SKILLSBENCH measures Skills efficacy, agent and  L2 (Basic): Adds overview paragraphs and one usage model contributions are hard to decouple. Developers often example (10%offull content). engineer commercial agents for specific models, especially  L3 (Full Skills): Complete environment/skills/ whendeveloped by the same organization. To isolate model directory with all examples, code snippets, and resources. contributions from harness effects, we adopt Terminus-2, a model-agnosticscaffoldfromTerminal-Bench(Merrilletal., 3.5. Evaluation Protocol 2026). Terminus-2 has a single tool, a headless terminal, andcompletestasksusingonlyBashcommands. Itprovides WeprovideSkillsassystem-level context preceding the task identical agent logic, tool interfaces, and Skills injection instruction in SKILLSBENCH. We list the injection format mechanisms across all models. and context management details in Appendix D. For each condition, the agent interacts with the containerized enviWeadditionally developed Terminus-2-Skills, a variant with ronment until task completion, timeout, or round limit. The explicit Skills-aware prompting that instructs agents to acverifier then executes deterministic assertions to produce a tively utilize provided Skills. However, this variant exhibits binary pass/fail outcome. higher exception rates (31.7-65.3% vs. 9.8-14.0%), revealing that Skills injection requires less naive agent scaffolding. 3.6. Metrics Andbecauseofthehighexception rate, we didn't choose to perform full experiments with this scaffold, as it would lead Pass Rate. Theprimary metric is pass rate: the fraction to little signal. More details are available in Appendix C. of tasks where the agent produces a final state passing all verification tests. We report pass rates with 95% bootstrap
3.2. Commercial and Open-source Agents confidence intervals across 5 runs.
Weevaluatethreecommercialcommand-lineagents(Claude Code(Anthropic, 2025b), Codex CLI (OpenAI, 2025), and GeminiCLI(Google,2025))andtwoTerminus-2variants Normalized Gain. Following Hake's formulation from (standard and Skills-aware) on SKILLSBENCH. physics education research (Hake, 1998), we define normal-
**Table 3. Pass rates (%) comparing no-Skills vs. with-Skills condiPareto (With Skills) Claude Haiku 4.5**
Pareto (Without Skills) Gemini 3 Pro tions across 85 tasks. 95% bootstrap CIs shown for . ConfiguraWith Skills Gemini 3 Flash 0.5 Without Skills GPT-5.2 Codex Claude Opus 4.5 MiniMax M2.1 tions ordered by overall pass rate. Claude Sonnet 4.5 Harness Model NoSkills WithSkills  95%CI 0.4 Codex GPT-5.2 41.8 49.5 +7.7 [4.2, 11.3] Claude Code Opus4.5 23.6 43.0 +19.3 [14.8, 23.9] ate (%) GeminiCLI Gemini3Flash 30.9 40.1 +9.2 [5.1, 13.4] ass R0.3 GeminiCLI Gemini3Pro 23.4 37.8 +14.3 [9.7, 19.0] P Claude Code Sonnet 4.5 12.5 27.2 +14.7 [10.1, 19.4] Claude Code Haiku 4.5 5.4 25.2 +19.9 [15.2, 24.5] 0.2 Claude Code MiniMax-M2.1 13.5 21.0 +7.5 [3.2, 11.9] Mean 21.6 34.8 +13.2 [10.4, 16.1] 0.1 10 1 100 101 Average Cost (cents, log scale) ized gain as: Figure 3. Pareto frontier of pass rate vs. cost across model-harness configurations. Filled markers indicate with-Skills conditions; pass pass hollow markers indicate without-Skills. Skills shift the Pareto g = skill vanilla (1) frontier upward, with Codex and Claude-Opus dominating the 1passvanilla with-Skills frontier. Interpreting Normalized Gain. Normalized gain has knownlimitations: a model scoring 90% vanilla and 95% Finding1: Skills provide substantial but variable benefit. with Skills yields g = 0.5, identical to a model scoring 10% Skills improve performance by +13.2pp on average across and 55%. These represent different phenomena (ceiling 7 commercial configurations (95% CI: [10.4, 16.1]), but effects vs. genuine scaffolding). We report both absolute imwith high variance across configurations (range: +7.5pp provement () and normalized gain (g) to enable nuanced to +19.9pp). This variability suggests that Skills efficacy interpretation. High g with low  suggests ceiling effects; depends strongly on the specific agent-model combination, high g with high  suggests substantial scaffolding. We contradicting the assumption of uniform Skills benefits. interpret the claim of "consistent scaffolding efficiency" as similar proportional benefit, not identical absolute improveFinding 2: Codex + GPT-5.2 achieves maximum perment. formance. The best-performing configuration is Codex CLI with GPT-5.2, achieving 49.5% pass rate with Skills Statistical Significance. WeapplyCochran's Q test for (46.0% overall). Notably, Claude Code with Opus 4.5 multi-condition comparisons, with post-hoc McNemar's achieves the highest improvement (+19.3pp), reflecting tests for pairwise differences. All reported improvements Claude Code's (Anthropic, 2025b) native Skills integrainclude p-values. tion optimized for the Agent Skills specification (Anthropic, 2025a).
## 4. Results
#### 4.1.2. HARNESS-SPECIFIC RELIABILITY
Ourresults are twofold: 1. main evaluation comparing SkillBeyond Skills efficacy, we observe reliability differences s-augmented vs. vanilla performance across 7 commercial across commercial harnesses: LLM-agentcombinations on 85 tasks, and 2. detailed analysis of Skills design factors including quantity, complexity,  Claude Code: Highest skills utilization rate; Claude domain effects, and Skills resolution ablations. models show largest Skills improvements (+14.7pp to +19.9pp).
4.1. Experiment 1: Skills Efficacy Across LLM-Agent  Codex CLI: Highest raw performance (49.5% with
Combinations Skills); frequently neglects provided Skills-agents acknowledge Skills content but often implement solutions Weevaluate how Skills improve agent performance across independently. commercial model-harness configurations. We test each  GeminiCLI:BalancedSkills utilization with moderate configuration with and without Skills on all 85 tasks. improvements (+9.2pp to +14.3pp).
4.1.1. MAIN RESULTS 4.1.3. DOMAIN-LEVEL ANALYSIS
Table 3 presents pass rates comparing vanilla (without Finding3: Skills can hurt performance in some domains. Skills) and Skills-augmented conditions across all modelTable 4 presents Skills efficacy by domain, revealing that harness combinations, ordered by overall performance. Skills do not uniformly benefit all domains. While Man-
**Table 4. Skills efficacy by domain across 85 tasks. Negative delta Table 5. Pass rates by number of Skills provided. 2-3 Skills shows**
indicates Skills hurt performance. optimal benefit. Domain WithSkills NoSkills  Skills Count WithSkills NoSkills  Manufacturing 32.6% 0.0% +32.6 1 skill 38.4% 26.8% +11.6 DocumentProcessing 55.9% 25.0% +30.9 2-3 skills 36.6% 16.6% +20.0 Security 42.7% 17.2% +25.5 4+skills 30.4% 25.2% +5.2 Financial 18.5% 0.9% +17.6 Multimedia 37.2% 22.0% +15.2 Data Processing 43.1% 29.3% +13.7 4.2.1. SKILLS QUANTITY ANALYSIS General 32.0% 20.1% +11.8 Planning/Optimization 29.4% 19.2% +10.2 Finding 4: 2-3 Skills is optimal; more Skills show diminControl Systems 31.2% 24.6% +6.6 ishing returns. Table 5 presents performance stratified by Healthcare 35.3% 33.3% +2.0 Scientific 32.2% 30.8% +1.4 number of Skills provided per task. Tasks with 2-3 Skills Software Engineering 30.0% 35.0% -5.0 show the largest improvement (+20.0pp), while 4+ Skills provides only +5.2pp benefit. This non-monotonic relationship suggests that excessive Skills content creates cognitive ufacturing (+32.6pp) and Document Processing (+30.9pp) overhead or conflicting guidance. benefit substantially, Software Engineering shows negative delta (-5.0pp) where Skills actually reduce perfor- 4.2.2. SKILLS COMPLEXITY ANALYSIS mance. This suggests that for domains where models have Table 6. Pass rates by Skills complexity level. Compact Skills strong pretraining coverage, external procedural guidance outperform detailed ones. mayconflict with internalized knowledge or impose unnecessary constraints. Complexity Pass Rate Skills  N Compact 28.5% +18.9 826
4.1.4. TASK-LEVEL ANALYSIS Detailed 28.3% +14.7 1165
Standard 35.7% +8.5 658 Analysis of 85 individual tasks reveals high variance in Comprehensive 17.4% +5.7 172 Skills effectiveness: Finding 5: Compact Skills outperform comprehensive TopSkills beneficiaries. Tasks showing largest improveones. Wepresent the effects of Skills documentation comments: manufacturing-fjsp-optimization plexity on performance in Table 6. Compact Skills (+18.9pp (+71.4pp, from 0% to 71.4%), delta) provide nearly 4 the benefit of comprehensive Skills sec-financial-report (+70.2pp), (+5.7pp). This suggests that focused, concise procedural offer-letter-generator (+64.5pp), guidance is more effective than exhaustive documentation- flood-risk-analysis (+59.2pp). These tasks agents may struggle to extract relevant information from involve specialized procedural knowledge rarely covered in lengthy Skills content. pretraining.
#### 4.2.3. MODEL SCALE EFFECTS
Skills hurt performance on some tasks. NoWestudytheeffects of the foundation models' scale across tably, 24 of 85 tasks show negative Skills Claude model family (Opus, Sonnet, Haiku 4.5). deltas: taxonomy-tree-merge (-50.0pp), fix-build-google-auto (-33.3pp), Finding 6: Smaller model + Skills can exceed larger multilingual-video-dubbing (-25.6pp), model without Skills. Claude Haiku 4.5 with Skills parallel-tfidf-search (-25.0pp). These failures (25.2%) outperforms Haiku without Skills (5.4%) by suggest Skills may introduce conflicting guidance or +19.9pp. Meanwhile, Claude Opus without Skills achieves unnecessary complexity for tasks models already handle 23.6%. This demonstrates that Skills can partially compenwell. sate for model capacity limitations on procedural tasks.
4.2. Experiment 2: Skills Design Factors 4.2.4. SKILLS RESOLUTION ABLATION
TounderstandhowSkillsdesignaffects efficacy, we analyze To isolate model contributions from harness effects, we the relationship between Skills quantity, complexity, and conduct ablation experiments using Terminus-2, a modelperformance. agnostic scaffold, across three Claude models on 27 hard tasks (54 runs per condition) with varying Skills resolution confirming that full Skills with scripts and references prolevels. videcritical procedural guidance. Whencontextistruncated,
**Table 7. Pass rates (%) by Skills resolution level (Terminus-2, 27 benefits diminish substantially-L3 achieves only +5.1pp**
hard tasks, 54 runs each). L0 = no Skills, BYOS = self-generated, over L0 (29.8% vs. 24.7%), suggesting that Skills efficacy L1=minimal,L2=basic,L3=full. 95%bootstrapCIsinparendepends critically on retaining full procedural content. theses.
## 5. Discussion
Model L0 BYOS L1 L2 L3 Opus4.5 11.1 (5.8, 16.4) 13.0 (7.2, 18.7) 22.2 (15.1, 29.4) 27.8 (20.1, 35.5) 40.7 (32.4, 49.1) Sonnet 4.5 9.3 (4.4, 14.1) 11.1 (5.8, 16.4) 16.7 (10.3, 23.0) 22.2 (15.1, 29.4) 35.2 (27.0, 43.3) Skills close procedural gaps. Skills are most helpful when Haiku 4.5 3.7 (0.5, 6.9) 5.6 (1.7, 9.4) 11.1 (5.8, 16.4) 14.8 (8.8, 20.9) 22.2 (15.1, 29.4) success depends on concrete procedures and verifier-facing Mean 8.0 (5.2, 10.9) 9.9 (6.8, 13.0) 16.7 (12.8, 20.5) 21.6 (17.3, 25.9) 32.7 (27.8, 37.6) details (steps, constraints, sanity checks), rather than broad conceptual knowledge. We observe large gains on domains Finding 7: Full Skills (L3) provide 25pp improvement with specialized workflows or brittle formats, and smaller over no Skills (L0). Table 7 shows progressive improveor negative effects when models already have strong priors mentacross resolution levels: L0  BYOS < L1 < L2  and the Skills adds overhead or conflicts. L3. The largest gain comes from L2L3 (+11.1pp), sugHarnesses mediate Skills use. Skills efficacy depends not gesting that executable code examples and references are only on Skills quality but also on how the harness implecritical Skills components. Opus shows the largest absolute mentsSkills. SomeharnessesreliablyretrieveanduseSkills, gain (L0: 11.1%  L3: 40.7%, +29.6pp), while the relative while others frequently acknowledge Skills content but proordering (Opus > Sonnet > Haiku) is preserved across all ceed without invoking them. Structured interfaces can also levels. introduce long-trajectory failure modes (e.g., format drift), reducing the influence of early-injected Skills. This motiFinding 8: Self-generated Skills (BYOS) perform near vates evaluating Skills under multiple harnesses rather than L0baseline. Whenpromptedtogeneratetheir own Skills treating "with Skills" as a single condition. before solving tasks, models achieve only +1.9pp over the no-Skills baseline (9.9% vs. 8.0%). Trajectory analysis Implications for Skills authoring. Ablations suggest that reveals two failure modes: (1) Models identify that doconcise, stepwise guidance with at least one working exammain-specific Skills are needed but generate imprecise or ple is often more effective than exhaustive documentation; incomplete procedures (e.g., listing "use pandas for data overly long Skills definition can increase context burden processing" without specific API patterns), and (2) For high without improving decisions. Modular Skills also appear domain-knowledgetasks(manufacturing, financial), models to compose better on multi-part tasks, and Skills should oftenfail to recognize the need for specialized Skills entirely, explicitly match harness constraints (e.g., repeated format attempting solutions with general-purpose approaches. This reminders for JSON-only protocols). suggests that effective Skills require human-curated domain expertise that models cannot reliably self-generate. 5.1. Limitations and Future Work Coverage and generalization. SkillsBench focuses on
4.2.5. CONTEXT USAGE terminal-based, containerized tasks for reproducible evaluation, so results may not directly transfer to GUI agents,
**Table 8. Context usage by Skills resolution (Terminus-2, Claude**
models). Truncation threshold: 100K tokens. multi-agent coordination, or very long-horizon workflows. Metric L0 BYOS L1 L2 L3 Wealsoevaluatealimitedsetofmodelsandharnesses;commercial harness behavior and Skills integration can change Valid runs 156 152 148 143 139 over time. A natural extension is to develop multi-modal Meantokens(K) 345.4 342.1 338.2 325.6 316.7 skills and protocols for vision-language agents operating in Truncation rate 57.1% 57.5% 58.1% 59.4% 60.4% Pass | truncated 24.7% 25.2% 25.9% 27.3% 29.8% GUIenvironments. Pass | not truncated 16.4% 18.5% 22.3% 32.2% 45.5% Causal attribution and controls. Skills injection increases context length, so observed gains could partly reflect "more Finding 9: Skills dramatically improve performance context" rather than procedural structure. Our perturbation when context is sufficient. Table 8 reveals progressive and BYOS controls suggest structure matters, but future improvement across resolution levels when context is not workrequires stronger length-matched baselines (e.g., rantruncated: L0 (16.4%)  BYOS (18.5%) < L1 (22.3%) < dom/irrelevant text and retrieval-only documentation conL2(32.2%) < L3 (45.5%). Self-generated Skills (BYOS) trols). These baselines also enable studying automatic provide minimal benefit (+2.1pp over L0), consistent with Skills synthesis from demonstrations or documentation and Finding 8. The largest gain comes from L2L3 (+13.3pp), isolating which Skills components (steps, examples, code resources) drive improvements. based on Terminal-Bench (Merrill et al., 2026) to separate Determinism, contamination, and ecological validity. model and harness effects. Finally, broader benchmarkContainerization provides state isolation but not perfect ing motivates careful reporting and comparability (Mattson determinism or immunity to training-set leakage. We mitiet al., 2020; Chiang et al., 2024; Srivastava et al., 2023); gate with multiple runs, a leakage audit (2.4.1), and paired wereport both absolute gains and normalized gain (Hake, (Skills vs. no Skills) comparisons, yet cannot eliminate 1998) to compare improvements across different baselines all nondeterminism or memorization effects. Future work (3.6). should evaluate ecosystem-representative settings, including lower-quality and automatically-selected Skills, and 7. Conclusion study Skills composition-when multiple Skills help or interfere, and whether composite performance can be preIn conclusion, we introduced SkillsBench, a benchmark dicted from atomic Skills effects. for evaluating Agent Skills as first-class artifacts. Across 85 tasks and 14 agent-model configurations, we find that
6. Related Work Skills often improve performance but the effect is highly
systemand domain-dependent: gains can be large, negligiSKILLSBENCHconnectstopriorworkon(1)benchmarking ble, or even negative. We further show that skill design and LLMagents,(2)augmenting agents with procedural knowlharness integration matter-concise, targeted Skills tend edge and tools, and (3) evaluating improvements across to work better than exhaustive ones, and aggressive skill heterogeneous systems. prompting can increase failure rates. Together, these results motivate evaluating augmentation as a paired effect (with Agent benchmarks. Recent benchmarks evaluate end-tovs. without skills), rather than assuming Skills are univerend agent capability across realistic environments, includsally beneficial, and provide a foundation for more rigorous ing Terminal-Bench (Merrill et al., 2026), SWE-bench and comparisons of Skills ecosystems, injection strategies, and follow-ons (Jimenez et al., 2024; Yang et al., 2024; 2025). future multimodal extensions. Broader environment coverage appears in AgentBench and interactive/web/GUI settings (Liu et al., 2023; Zhou et al., ImpactStatement 2024b; Koh et al., 2024; Xie et al., 2024). Other suites emphasize tool-mediated workflows, interactive execution This paper presents work whose goal is to advance the field feedback, or domain specialization (Yao et al., 2025; Trivedi of Machine Learning through improved evaluation methodet al., 2024; Yang et al., 2023; Chan et al., 2025; Zhang ology for AI agents. We see no immediate ethical concerns et al., 2024; Zhuo et al., 2025; Austin et al., 2021; Ye et al., beyond those inherent to agent systems generally. By en- 2025). These benchmarks measure how well a fixed agent abling better evaluation of agent augmentation strategies, completes tasks. SKILLSBENCH instead measures augmenour work may contribute to more reliable and capable AI tation efficacy via paired evaluation. assistants. Procedural augmentation and tool use. Prior work augments agents with structured reasoning or external knowlReferences edge, e.g., CoALA and Voyager (Sumers et al., 2023; Wang et al., 2023a), chain-of-thought and ReAct for multi-step Anthropic. Introducing the model context protocol. https: problem solving (Wei et al., 2022; Yao et al., 2023; 2022; //www.anthropic.com/news/model-conte Shinn et al., 2023; Madaan et al., 2023; Zhou et al., 2023; xt-protocol, November 2024. Open standard for 2024a), and retrieval/tool use (Lewis et al., 2020; Zhou et al., connecting AI systems with data sources. 2022; Schick et al., 2023; Qin et al., 2024), and declarative optimization frameworks (Khattab et al., 2023). Skills comAnthropic. Equipping agents for the real world with agent bine procedural guidance with executable resources (2.1). skills. https://www.anthropic.com/engi Despite many augmentation methods, benchmarks rarely neering/equipping-agents-for-the-rea quantify their actual impact. l-world-with-agent-skills, October 2025a. Anthropic Engineering Blog. Skills ecosystems and evaluation methodology. Anthropic's Agent Skills and MCP specifications (Anthropic, Anthropic. Claude code: an agentic coding tool. https: 2025a; 2024) formalized skill packages and tool connec- //github.com/anthropics/claude-code, tivity, while agent CLIs (Claude Code, Gemini CLI, and 2025b. Codex) provide real-world harnesses (Anthropic, 2025b; Google, 2025; OpenAI, 2025). SKILLSBENCH evaluates Anthropic. Demystifying evals for AI agents. https:// both commercial harnesses and a model-agnostic harness www.anthropic.com/engineering/demyst ifying-evals-for-ai-agents, January 2026. 2024. URL https://openreview.net/forum Anthropic Engineering Blog. ?id=VTF8yNQM66. Austin, J., Odena, A., Nye, M., Bosma, M., Michalewski, Khattab, O., Singhvi, A., Maheshwari, P., Zhang, Z., SanH., Dohan, D., Jiang, E., Cai, C., Terry, M., Le, Q., et al. thanam, K., Vardhamanan, S., Haq, S., Sharma, A., Joshi, Program synthesis with large language models. arXiv T. T., Moazam, H., et al. Dspy: Compiling declarative preprint arXiv:2108.07732, 2021. language model calls into self-improving pipelines. arXiv Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., preprint arXiv:2310.03714, 2023. Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Koh, J. Y., Lo, R., Jang, L., Duvvur, V., Lim, M., Huang, Askell, A., et al. Language models are few-shot learners. P.-Y., Neubig, G., Zhou, S., Salakhutdinov, R., and Fried, Advances in neural information processing systems, 33: D. Visualwebarena: Evaluating multimodal agents on 1877-1901, 2020. realistic visual web tasks. In Proceedings of the 62nd Annual Meeting of the Association for Computational Brown,W. Verifiers: Environments for llm reinforcement Linguistics (Volume 1: Long Papers), pp. 881-905, 2024. learning. https://github.com/PrimeIntell ect-ai/verifiers,2025. Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Chan,J.S.,Chowdhury,N.,Jaffe,O.,Aung,J.,Sherburn,D., Goyal, N., Kttler, H., Lewis, M., Yih, W.-t., Rocktschel, Mays, E., Starace, G., Liu, K., Maksin, L., Patwardhan, T., et al. Retrieval-augmented generation for knowledgeT., Madry, A., and Weng, L. MLE-bench: Evaluating intensive nlp tasks. Advances in neural information promachinelearningagentsonmachinelearningengineering. cessing systems, 33:9459-9474, 2020. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview Liu, X., Yu, H., Zhang, H., Xu, Y., Lei, X., Lai, H., Gu, Y., .net/forum?id=6s5uXNWGIh. Ding, H., Men, K., Yang, K., et al. Agentbench: Evaluating llms as agents. arXiv preprint arXiv:2308.03688, Chiang, W.-L., Zheng, L., Sheng, Y., Angelopoulos, A. N., 2023. Li, T., Li, D., Zhu, B., Zhang, H., Jordan, M., Gonzalez, J. E., et al. Chatbot arena: An open platform for evaluatMadaan, A., Tandon, N., Gupta, P., Hallinan, S., Gao, ing llms by human preference. In Forty-first International L., Wiegreffe, S., Alon, U., Dziri, N., Prabhumoye, S., Conference on Machine Learning, 2024. Yang, Y., et al. Self-refine: Iterative refinement with selffeedback. Advances in Neural Information Processing Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, Systems, 36:46534-46594, 2023. G., Roberts, A., Barham, P., Chung, H. W., Sutton, C., Gehrmann, S., et al. Palm: Scaling language modeling Mattson, P., Cheng, C., Diamos, G., Coleman, C., Micikewith pathways. Journal of Machine Learning Research, vicius, P., Patterson, D., Tang, H., Wei, G.-Y., Bailis, P., 24(240):1-113, 2023. URL https://dl.acm.org Bittorf, V., et al. Mlperf training benchmark. Proceedings /doi/10.5555/3648699.3648939. of Machine Learning and Systems, 2:336-349, 2020. Google. Gemini cli: An open-source ai agent that brings Merrill, M. A., Shaw, A. G., Carlini, N., Li, B., Raj, H., the power of gemini directly into your terminal. https: Bercovich, I., Shi, L., Shin, J. Y., Walshe, T., Buchanan, //github.com/google-gemini/gemini-cli, E. K., Shen, J., Ye, G., Lin, H., Poulos, J., Wang, M., 2025. Nezhurina, M., Jitsev, J., Lu, D., Mastromichalakis, O. M., Xu, Z., Chen, Z., Liu, Y., Zhang, R., Chen, L. L., Hake, R. R. Interactive-engagement versus traditional methKashyap, A., Uslu, J.-L., Li, J., Wu, J., Yan, M., Bian, ods: A six-thousand-student survey of mechanics test S., Sharma, V., Sun, K., Dillmann, S., Anand, A., Landata for introductory physics courses. American journal pouthakoun, A., Koopah, B., Hu, C., Guha, E., Dreiman, of Physics, 66(1):64-74, 1998. G. H. S., Zhu, J., Krauth, K., Zhong, L., Muennighoff, N., Amanfu, R., Tan, S., Pimpalgaonkar, S., Aggarwal, Harbor Framework Team. Harbor: A framework for evalT., Lin, X., Lan, X., Zhao, X., Liang, Y., Wang, Y., Wang, uating and optimizing agents and models in container Z., Zhou, C., Heineman, D., Liu, H., Trivedi, H., Yang, environments., 2026. URL https://github.com J., Lin, J., Shetty, M., Yang, M., Omi, N., Raoof, N., Li, /laude-institute/harbor. S., Zhuo, T. Y., Lin, W., Dai, Y., Wang, Y., Chai, W., Jimenez, C. E., Yang, J., Wettig, A., Yao, S., Pei, K., Press, Zhou, S., Wahdany, D., She, Z., Hu, J., Dong, Z., Zhu, O., and Narasimhan, K. R. SWE-bench: Can language Y., Cui, S., Saiyed, A., Kolbeinsson, A., Hu, J., Rytting, models resolve real-world github issues? In The Twelfth C. M., Marten, R., Wang, Y., Dimakis, A., Konwinski, A., International Conference on Learning Representations, and Schmidt, L. Terminal-bench: Benchmarking agents on hard, realistic tasks in command line interfaces, 2026. Trivedi, H., Khot, T., Hartmann, M., Manku, R., Dong, URLhttps://arxiv.org/abs/2601.11868. V., Li, E., Gupta, S., Sabharwal, A., and Balasubramanian, N. AppWorld: A controllable world of apps and OpenAI. Codex cli: Lightweight coding agent that runs in people for benchmarking interactive coding agents. In your terminal. https://github.com/openai/ Ku, L.-W., Martins, A., and Srikumar, V. (eds.), Procodex,2025. ceedings of the 62nd Annual Meeting of the AssociaOuyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., tion for Computational Linguistics (Volume 1: Long Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., Papers), pp. 16022-16076, Bangkok, Thailand, August et al. Training language models to follow instructions 2024. Association for Computational Linguistics. doi: with human feedback. Advances in neural information 10.18653/v1/2024.acl-long.850. URL https: processing systems, 35:27730-27744, 2022. //aclanthology.org/2024.acl-long.850/. Wang,G.,Xie,Y., Jiang, Y., Mandlekar, A., Xiao, C., Zhu, Qin, Y., Liang, S., Ye, Y., Zhu, K., Yan, L., Lu, Y., Lin, Y., Fan, L., and Anandkumar, A. Voyager: An openY., Cong, X., Tang, X., Qian, B., Zhao, S., Hong, L., endedembodiedagentwithlargelanguagemodels. arXiv Tian, R., Xie, R., Zhou, J., Gerstein, M., li, d., Liu, Z., preprint arXiv:2305.16291, 2023a. and Sun, M. Toolllm: Facilitating large language models to master 16000+ real-world apis. In Kim, B., Yue, Y., Wang, Z., Zhou, S., Fried, D., and Neubig, G. ExecutionChaudhuri, S., Fragkiadaki, K., Khan, M., and Sun, Y. based evaluation for open-domain code generation. In (eds.), International Conference on Learning RepresentaFindings of the Association for Computational Linguistions, volume 2024, pp. 9695-9717, 2024. URL https: tics: EMNLP 2023, pp. 1271-1290, 2023b. //proceedings.iclr.cc/paper_files/pa Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, per/2024/file/28e50ee5b72e90b50e7196 E., Le, Q. V., Zhou, D., et al. Chain-of-thought prompting fde8ea260e-Paper-Conference.pdf. elicits reasoning in large language models. Advances in neural information processing systems, 35:24824-24837, Schick, T., Dwivedi-Yu, J., Dess, R., Raileanu, R., Lomeli, 2022. M., Hambro, E., Zettlemoyer, L., Cancedda, N., and Scialom, T. Toolformer: Language models can teach Xie, T., Zhang, D., Chen, J., Li, X., Zhao, S., Cao, R., Hua, themselves to use tools. Advances in Neural Information T. J., Cheng, Z., Shin, D., Lei, F., et al. Osworld: BenchProcessing Systems, 36:68539-68551, 2023. marking multimodal agents for open-ended tasks in real computer environments. Advances in Neural Information Shinn, N., Cassano, F., Gopinath, A., Narasimhan, K., and Processing Systems, 37:52040-52094, 2024. Yao, S. Reflexion: Language agents with verbal reinforcement learning. Advances in Neural Information Yang, J., Prabhakar, A., Narasimhan, K., and Yao, S. InterProcessing Systems, 36:8634-8652, 2023. code: Standardizing and benchmarkinginteractive coding withexecutionfeedback. AdvancesinNeuralInformation Srivastava, A., Rastogi, A., Rao, A., Shoeb, A. A. M., Abid, Processing Systems, 36:23826-23854, 2023. A., Fisch, A., Brown, A. R., Santoro, A., Gupta, A., Yang, J., Jimenez, C. E., Wettig, A., Lieret, K., Yao, Garriga-Alonso, A., et al. Beyond the imitation game: S., Narasimhan, K., and Press, O. Swe-agent: AgentQuantifyingandextrapolatingthecapabilitiesoflanguage computer interfaces enable automated software engineermodels. Transactions on machine learning research, ing. Advances in Neural Information Processing Systems, 2023. 37:50528-50652, 2024. Sumers, T., Yao, S., Narasimhan, K. R., and Griffiths, T. L. Yang, J., Lieret, K., Jimenez, C. E., Wettig, A., Khandpur, Cognitive architectures for language agents. Transactions K., Zhang, Y., Hui, B., Press, O., Schmidt, L., and Yang, onMachineLearningResearch, 2023. D. Swe-smith: Scaling data for software engineering Sutton, R. S., Precup, D., and Singh, S. Between mdps agents. In Proceedings of the 39th Annual Conference on and semi-mdps: A framework for temporal abstraction in Neural Information Processing Systems (NeurIPS 2025 reinforcement learning. Artificial intelligence, 112(1-2): D&BSpotlight), 2025. URL https://arxiv.or 181-211, 1999. g/abs/2504.21798. arXiv:2504.21798,accepted at NeurIPS 2025 (Spotlight). Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, Yao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan, M.-A., Lacroix, T., Rozire, B., Goyal, N., Hambro, E., K. R., and Cao, Y. React: Synergizing reasoning and Azhar, F., et al. Llama: Open and efficient foundation lanacting in language models. In The eleventh international guage models. arXiv preprint arXiv:2302.13971, 2023. conference on learning representations, 2022. Yao, S., Yu, D., Zhao, J., Shafran, I., Griffiths, T., Cao, Y., A., Zaharia, M., Stoica, I., Liang, P., and Kang, D. EstabandNarasimhan,K. Treeofthoughts: Deliberateproblem lishing best practices in building rigorous agentic benchsolving with large language models. Advances in neural marks. In The Thirty-ninth Annual Conference on Neuinformation processing systems, 36:11809-11822, 2023. ral Information Processing Systems Datasets and Benchmarks Track, 2025. URL https://openreview.n Yao, S., Shinn, N., Razavi, P., and Narasimhan, K. R. - et/forum?id=E58HNCqoaA. bench: A benchmark for tool-agent-user interaction in real-world domains. In The Thirteenth International Zhuo, T. Y., Chien, V. M., Chim, J., Hu, H., Yu, W., Conference on Learning Representations, 2025. URL Widyasari, R., Yusuf, I. N. B., Zhan, H., He, J., Paul, https://openreview.net/forum?id=roNS I., Brunner, S., GONG, C., Hoang, J., Zebaze, A. R., XZpUDN. Hong, X., Li, W.-D., Kaddour, J., Xu, M., Zhang, Z., Yadav, P., Jain, N., Gu, A., Cheng, Z., Liu, J., Liu, Q., Wang, Ye, C., Yuan, S., Cooray, S., Dillmann, S., Roque, I. L. V., Z., Lo, D., Hui, B., Muennighoff, N., Fried, D., Du, X., Baron, D., Frank, P., Martin-Alvarez, S., Koblischke, N., deVries,H.,andWerra,L.V. Bigcodebench: BenchmarkQu,F.J., Yang, D., Wechsler, R., and Ciuca, I. Replicaing code generation with diverse function calls and comtionbench: Can ai agents replicate astrophysics research plex instructions. In The Thirteenth International Conferpapers?, 2025. URL https://arxiv.org/abs/ ence on Learning Representations, 2025. URL https: 2510.24591. //openreview.net/forum?id=YrycTjllL0. Zhang, A. K., Perry, N., Dulepet, R., Ji, J., Menders, C., Lin, J. W., Jones, E., Hussein, G., Liu, S., Jasper, D., et al. Cybench: A framework for evaluating cybersecurity capabilities and risks of language models. arXiv preprint arXiv:2408.08926, 2024. Zhou, A., Yan, K., Shlapentokh-Rothman, M., Wang, H., and Wang, Y.-X. Language agent tree search unifies reasoning acting and planning in language models. In International Conference on Machine Learning (ICML), 2024a. arXiv:2310.04406. Zhou,D.,Schrli, N., Hou, L., Wei, J., Scales, N., Wang, X., Schuurmans, D., Cui, C., Bousquet, O., Le, Q., and Chi,
## E. Least-to-most prompting enables complex reasoning
in large language models. In The Eleventh International Conference on Learning Representations (ICLR), 2023. arXiv:2205.10625. Zhou, S., Alon, U., Xu, F. F., Jiang, Z., and Neubig, G. Docprompting: Generating code by retrieving the docs. In The Eleventh International Conference on Learning Representations, 2022. Zhou, S., Xu, F. F., Zhu, H., Zhou, X., Lo, R., Sridhar, A., Cheng, X., Ou, T., Bisk, Y., Fried, D., Alon, U., and Neubig, G. Webarena: A realistic web environment for building autonomous agents. In The Twelfth International Conference on Learning Representations, 2024b. URL https://openreview.net/forum?id=oKn9 c6ytLx. Zhu, Y., Jin, T., Pruksachatkun, Y., Zhang, A. K., Liu, S., Cui, S., Kapoor, S., Longpre, S., Meng, K., Weiss, R., Barez, F., Gupta, R., Dhamala, J., Merizian, J., Giulianelli, M., Coppock, H., Ududec, C., Kellermann, A., Sekhon,J.S.,Steinhardt, J., Schwettmann, S., Narayanan,
## A. Skill Ecosystem Analysis
Tocontextualize SKILLSBENCH within the broader landscape of agent augmentation, we analyze the existing ecosystem of publicly available Skills. A.1. Data Collection Wecollected Skills from three sources:
- Public GitHub repositories tagged with "claude-skills" or "agent-skills" (N=12847)
- Communitymarketplaces including Smithery.ai and skillmp.com (N=28412)
- Corporate partner contributions (N=5891)
After deduplication (based on SKILL.md content hash), we retained 47150 unique Skills from 6323 repositories. Total: 84,192 17,500 Peak: 18,904 (Jan 22) 80,000 Avg daily: 810 Duration: 136 days 70,000 15,000 Daily count Cumulative count 60,000 12,500 50,000 10,000 40,000 7,500 Daily Skill Count 30,000 5,000 20,000 Cumulative Skill Count 2,500 10,000 0 0 Oct 07 Oct 21 Jan 13 Jan 27 Sep 09 Sep 23 Nov 04 Nov 18 Dec 02 Dec 16 Dec 30 Date
**Figure 4. Temporal dynamics of Skill creation over 136 days. Daily additions (bars, left axis) remained modest through late 2025, then**
surged to a peak of 18904 in January 2026. The cumulative curve (line, right axis) reflects exponential-like growth, reaching 84192 total Skills. A.2. Skill Characteristics Size Distribution. Skill sizes follow a log-normal distribution with median 2.3 KB (IQR: 0.8-6.1 KB). The largest Skills (top 1%) exceed 50 KB and typically include extensive code resources. Figure 5 shows the SKILL.md token distribution, and Figure 6 shows total Skill size distribution. DomainCoverage. Skillsspandiverse domains (Figure 7):
- Software Development: 38% (git workflows, code review, testing)
- Data Analysis: 22% (pandas, SQL, visualization)
- DevOps/Infrastructure: 15% (Docker, Kubernetes, CI/CD)
- Writing/Documentation: 12% (technical writing, API docs)
- Other: 13% (scientific computing, finance, etc.)
104 Median = 1,569 Mean = 2,087 Log-normal fit Std = 2,653 Min = 44 103 Max = 256,316 Frequency (log scale) 0 2,000 4,000 6,000 8,000 10,000 12,000 Token Count (1 char 0.25 tokens)
**Figure 5. Token distribution of SKILL.md files (n=36338, 99.5th percentile shown). Most Skills are lightweight with median 1.5k**
tokens. Structural Patterns. MostSkills (78%) follow the standard structure with SKILL.md plus optional resources. Figure 8 shows that most Skills contain very few files (median of one, concentrated below five). Figure 9 confirms the ecosystem is documentation-heavy: markdown files dominate, followed by scripting and configuration code. A.3. Quality Indicators Wedevelopedaquality scoring rubric based on:
1. Completeness: Presence of required components (0-3 points)
## 2. Clarity: Readability and organization (0-3 points)
3. Specificity: Actionable vs. vague guidance (0-3 points)
4. Examples: Presence and quality of examples (0-3 points)
Meanquality score across the ecosystem is 6.2/12 (SD=2.8), indicating substantial room for improvement in Skill authoring practices. A.4. Implications for Benchmark Design This ecosystem analysis directly informed SKILLSBENCH construction:
- Domainselection: Task categories mirror ecosystem coverage, ensuring Skills exist for evaluationQuality awareness: Ecosystem mean quality of 6.2/12 motivated our leakage audit and authoring guidelines-lowquality Skills would confound efficacy measurement
Median = 2,296 Mean = 11,898 Log-normal fit Std = 220,315 Min = 10 Max = 20,866,120 Frequency (log scale) 0 20,000 40,000 60,000 80,000 100,000120,000140,000160,000 Total Token Count (1 char 0.25 tokens)
**Figure 6. Total Skill size distribution (n=37078, 99.5th percentile shown, excluding metadata.json). Median total size remains under 2.5k**
tokens, with distribution highly skewed toward concise artifacts.
- Skill selection: We selected benchmark Skills from the top quality quartile (score  9/12) to isolate the effect of
procedural knowledge from Skill quality varianceSize constraints: Median Skill size (800 tokens) informed our 8K context budget allocation
Limitation: Benchmark vs. Ecosystem Gap. Our 85 curated tasks with high-quality Skills represent an optimistic scenario. Real-world Skill usage involves lower-quality Skills (ecosystem mean: 6.2/12 vs. benchmark mean: 10.1/12) and imperfect Skill-task matching. Future work should evaluate with ecosystem-representative Skill samples.
## B. Qualitative Case Studies
Wepresent three case studies illustrating Skill efficacy patterns. B.1. Case 1: Successful Skill Application Task: Fix a failing CI pipeline in a Python repository (Medium difficulty, DevOps domain). Skill: ci-debugging-providessystematicdebuggingworkflow: (1) check logs, (2) identify failing step, (3) reproduce locally, (4) trace dependencies. Vanilla trajectory (GPT-5.2): Agent immediately attempts to modify .github/workflows/main.yml based on error message, introduces syntax error, enters retry loop, times out after 47 minutes. Skill-augmented trajectory: Agent follows Skill procedure-reads full log, identifies dependency version conflict, checks requirements.txt,findspinnedversionincompatibility, updates version constraint, verifies fix locally before committing. Completes in 12 minutes. Analysis: Skill provided systematic procedure that prevented premature action. Key Skill component: explicit "reproduce locally before modifying" instruction. Other (combined) 20.5% Documentation 11.9% Git/Version Control 11.8% Code Quality 9.0% CLI/Shell 8.0% DevOps/Infrastructure 7.8% API 7.4% Testing 6.5% Frontend/UI 5.9% JavaScript/TypeScript 5.8% Other 5.4% n = 11 categories 0% 5% 10% 15% 20% Percentage of Skills (%)
**Figure 7. Distribution of Skill categories. The top 10 categories account for 79.6% of all Skills, with Documentation (11.9%), Git/Version**
Control(11.8%),andCodeQuality(9.0%)leading. Nosinglecategorydominates,reflectingdiversedeveloperneedsacrossdocumentation, infrastructure, testing, and frontend tasks. B.2. Case 2: Skill Present but Ignored Task: Implement OAuth authentication flow (Hard difficulty, Software Engineering domain). Skill: oauth-integration-providesflowdiagram,commonpitfalls,security checklist. Trajectory (Claude Haiku 4.5): Agent acknowledges Skill in first response ("I'll follow the oauth-integration guidelines...") but subsequently ignores security checklist, implements vulnerable token storage, fails verification. Analysis: The agent recognized the Skill but did not operationalize it. Smaller models may lack capacity to maintain Skill guidance across long trajectories. Suggests need for Skill-aware prompting strategies. B.3. Case 3: Skill Quality Impact Task: Configure Kubernetes deployment (Medium difficulty, DevOps domain). Skill A (high quality, score 11/12): Step-by-step procedure with YAML examples, common errors, verification commands. Skill B (low quality, score 5/12): High-level overview, no examples, vague guidance ("configure the deployment appropriately"). Results (Claude Sonnet 4.5): Skill A: 87.5% pass rate. Skill B: 54.2% pass rate (below vanilla: 58.3%). Analysis: Low-quality Skills can hurt performance by consuming context budget without providing actionable guidance. Highlights importance of Skill quality control.
## C. Experimental Setup Details
This appendix provides full details of the experimental setup summarized in 3. 25,000 Median = 1 Mean = 3.85 Log-normal fit Std = 31.14 20,000 Min = 0 Max = 3,206 15,000 10,000 Number of Skills 5,000 0 5 10 15 20 25 30 Number of Files (excluding metadata.json)
**Figure 8. File count distribution per Skill. Most Skills contain 1-5 files.**
C.1. Model and Harness Configurations Table 9 presents all 7 commercial agent-model configurations evaluated in the main experiments.
**Table 9. Agent harnesses and models evaluated in main experiments. Total: 2857 valid trajectories across 7 configurations. Additional**
Terminus-2 configurations used for ablation experiments only (4). Harness Model Provider Runs Opus4.5 Anthropic 240 Claude Code Sonnet 4.5 Anthropic 236 Haiku 4.5 Anthropic 252 MiniMax-M2.1 MiniMax 215 GeminiCLI Gemini3Pro Google 308 Gemini3Flash Google 315 Codex GPT-5.2 OpenAI 350 C.2. Harness Descriptions CommercialHarnesses. Weevaluatethreecommercialagentharnesses:
- Claude Code (Anthropic, 2025b): Anthropic's agent with native Skill integrationGeminiCLI(Google,2025): Google's open-source terminal agentCodexCLI(OpenAI,2025): OpenAI'slightweight coding agent
These tightly couple specific models with proprietary agent logic, representing real-world deployment conditions but confounding model and harness effects. .md 92,760 .py 10,115 .ts 3,917 .sh 3,871 .json 3,823 .js 2,854 .xsd 2,075 .tsx 1,978 .txt 1,859 .yml 1,800 .html 1,742 .yaml 1,425 .jsonl 1,307 .vue 1,128 .cs 1,112 .ttf 1,065 (no extension) 1,048 .meta 765 .mp3 721 .png 670 Top 20 of 286 extensions 0 20,000 40,000 60,000 80,000 100,000 Number of Files
**Figure 9. File extension distribution. Markdown files dominate, indicating Skills prioritize natural-language instructions over executable**
implementations. Terminus-2 (Decoupled Harness). Toisolate model contributions, we use Terminus-2, a model-agnostic harness based on Terminal-Bench (Merrill et al., 2026).1 Terminus-2 provides identical agent logic, tool interfaces, and Skill injection across all models. Terminus-2-Skills (Skill-Aware Variant). This variant uses explicit Skill-aware prompting that instructs agents to actively utilize provided Skills. It exhibits higher exception rates (31.7-65.3% vs. 9.8-14.0% for standard Terminus-2), revealing that aggressive Skill prompting can destabilize execution. ModelFamilyConsideration. ClaudemodelshavebeentrainedwithawarenessoftheAgentSkillsspecification (Anthropic, 2025a), which may confer advantages when processing Skill-formatted instructions. C.3. Agent Interface Agents interact with the environment through a standardized interface: class BaseAgent(ABC): @abstractmethod def step(self, obs: str) -> str: """obs: terminal output -> action""" pass 1Implementation: https://github.com/laude-institute/terminal-bench/tree/main/terminal_bench/ agents/terminus_2 C.4. Skill Injection Details SkillsBench provides standardized Skill injection across harnesses:
- Commercialharnesses: Skills injected via native configuration mechanismsTerminus-2: Skills loaded into system prompt with configurable truncation levels
C.5. Inference ConfigurationTemperature: 0 (deterministic sampling)
- Maxrounds: Level-dependent (10/30/50 for Easy/Medium/Hard)
- Context management: Sliding window with 8K token limit; oldest turns dropped when exceededTimeout: Per-task, specified in task.toml (default: 30 min)
Weruneachmodel-harness-condition combination at least 5 times per task. We report mean pass rates with 95% bootstrap confidence intervals. This yields 85  5 = 425 runs per condition per model-harness pair. Including ablation experiments, the full evaluation comprises 2857 valid trajectories. C.6. Ablation Study Designs WeconductaSkills resolution ablation study using the Terminus-2 scaffold: Skills Resolution Ablation. WeuseTerminus-2withallthree Claude models (Haiku, Sonnet, Opus 4.5) across 5 Skills resolution levels (L0, BYOS, L1, L2, L3) on 27 hard tasks. This design isolates model scale effects within a single family while controlling for harness variation. Results are presented in 4. Future Ablation Directions. Additional ablation studies we plan to conduct include:
- Instruction Specificity: Varying detail level from minimal to full SOP with worked examplesSkill Granularity: Comparing monolithic vs. modular vs. retrieved SkillsPerturbation Robustness: Testing robustness to typos, reordering, and paraphrasing
## D. Additional Experimental Details
D.1. Context Usage Analysis Table 10 provides detailed context usage statistics by condition.
**Table 10. Detailed context usage by Skill condition (Terminus-2 ablation, 27 hard tasks).**
Metric L0 BYOS L1 L2 L3 Meantokens 4821 5891 5102 5487 6142 Std. dev. 1203 1723 1342 1518 1847 Truncation rate 8.3% 13.1% 9.7% 11.4% 14.2% D.2. Skill Injection Format For Gemini CLI, Codex, and Claude Code, we use their built-in skill implementation systems. For other harnesses like Terminus-2, we inject Skills as system-level context using this structure: <skill name="[NAME]"> <instructions>[SKILL.md content]</instructions> <resources>[file list]</resources> </skill> [Task instruction follows] D.3. Confidence Interval Calculation Wecompute95%confidenceintervals using the percentile bootstrap method with 1000 resamples. For normalized gain, we compute CIs on the gain metric directly rather than on the component pass rates. D.4. 10-Run Validation For a subset of configurations (GPT-5.2 and Claude Opus 4.5, all 27 hard tasks, L0 and L3 conditions), we conducted 10 runs instead of 5. Results show:
- Meanwithin1.2ppof5-runestimates
1001  Standard error reduced by 30% 1002  All conclusions remain unchanged 1005 Thisvalidates that 5 runs provides sufficient precision for our main findings.